apiVersion: template.openshift.io/v1
kind: Template
labels:
  app: ${NAME}-${ZONE}
parameters:
  - name: NAME
    description: Product name
    value: nr-spar
  - name: COMPONENT
    description: Component name
    value: fluentbit
  - name: ZONE
    description: Deployment zone, e.g. pr-### or prod
    required: true
  - name: AWS_KINESIS_STREAM
    description: AWS Kinesis Stream identifier
    required: false
  - name: AWS_KINESIS_ROLE_ARN
    description: AWS OpenSearch/Kinesis Resource Name
    required: false
  - name: FLUENT_CONF_HOME
    description: FluentBit configuration home
    value: "/fluent-bit/etc"
  - name: FLUENT_VERSION
    description: FluentBit version (docker tag). You may include a debug version.
    value: "2.1"
  - name: LOGGING_CPU_LIMIT
    description: Limit Peak CPU per pod (in millicores ex. 1000m)
    displayName: CPU Limit
    value: 100m
  - name: LOGGING_CPU_REQUEST
    description: Requested CPU per pod (in millicores ex. 500m)
    displayName: CPU Request
    value: 10m
  - name: LOGGING_MEMORY_LIMIT
    description: Limit Peak Memory per pod (in gigabytes Gi or megabytes Mi ex. 2Gi)
    displayName: Memory Limit
    value: 64Mi
  - name: LOGGING_MEMORY_REQUEST
    description: Requested Memory per pod (in gigabytes Gi or megabytes Mi ex. 500Mi)
    displayName: Memory Request
    value: 16Mi
  - name: MIN_REPLICAS
    description: Dummy value for workflow convenience
  - name: MAX_REPLICAS
    description: Dummy value for workflow convenience
objects:
  - kind: DeploymentConfig
    apiVersion: v1
    metadata:
      labels:
        app: ${NAME}-${ZONE}
      name: ${NAME}-${ZONE}-${COMPONENT}
    spec:
      replicas: 1
      selector:
        deploymentconfig: ${NAME}-${ZONE}-${COMPONENT}
      strategy:
        type: Rolling
      template:
        metadata:
          labels:
            app: ${NAME}-${ZONE}
            deploymentconfig: ${NAME}-${ZONE}-${COMPONENT}
        spec:
          containers:
            - name: ${NAME}
              image: docker.io/fluent/fluent-bit:${FLUENT_VERSION}
              imagePullPolicy: Always
              ports:
                - containerPort: 2020
                  name: metrics
                  protocol: TCP
                - containerPort: 80
                  name: http-plugin
                  protocol: TCP
              resources:
                requests:
                  cpu: "${LOGGING_CPU_REQUEST}"
                  memory: "${LOGGING_MEMORY_REQUEST}"
                limits:
                  cpu: "${LOGGING_CPU_LIMIT}"
                  memory: "${LOGGING_MEMORY_LIMIT}"
              env:
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      name: ${NAME}-${ZONE}-fluentbit
                      key: aws-access-key-id
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      name: ${NAME}-${ZONE}-fluentbit
                      key: aws-access-key-secret
                - name: STREAM_NAME
                  valueFrom:
                    secretKeyRef:
                      name: ${NAME}-${ZONE}-fluentbit
                      key: aws-kinesis-stream
                - name: ROLE_ARN
                  valueFrom:
                    secretKeyRef:
                      name: ${NAME}-${ZONE}-fluentbit
                      key: aws-kinesis-role-arn
                - name: FLUENT_CONF_HOME
                  value: ${FLUENT_CONF_HOME}
                - name: FLUENT_VERSION
                  value: ${FLUENT_VERSION}
                - name: AGENT_NAME
                  value: ${NAME}-${ZONE}
              volumeMounts:
                - name: ${NAME}-${ZONE}-${COMPONENT}-logs
                  mountPath: /logs
                - name: ${NAME}-${ZONE}-${COMPONENT}-configs
                  mountPath: ${FLUENT_CONF_HOME}
                # TLS cert for connecting to fluentd (enable for TLS)
                # - mountPath: /fluent-bit/ssl
                #   name: fluent-bit-tls
          volumes:
            - name: ${NAME}-${ZONE}-${COMPONENT}-logs
              persistentVolumeClaim:
                claimName: ${NAME}-${ZONE}-${COMPONENT}-logs
            - name: ${NAME}-${ZONE}-${COMPONENT}-configs
              configMap:
                name: ${NAME}-${ZONE}-${COMPONENT}-configs
                items:
                  - key: filters.conf
                    path: filters.conf
                  - key: fluent-bit.conf
                    path: fluent-bit.conf
                  - key: generic_json_parsers.conf
                    path: generic_json_parsers.conf
                  - key: host_metadata.lua
                    path: host_metadata.lua
                  - key: outputs.conf
                    path: outputs.conf
                  - key: parsers.conf
                    path: parsers.conf
                  - key: spar_filter_filters.conf
                    path: spar/filter/filters.conf
                  - key: spar-oracle_filter_filters.conf
                    path: spar-oracle/filter/filters.conf
                  - key: spar_input_inputs.conf
                    path: spar/input/inputs.conf
                  - key: spar-oracle_input_inputs.conf
                    path: spar-oracle/input/inputs.conf
                  - key: spar_parser_parsers.conf
                    path: spar/parser/parsers.conf
                  - key: spar-oracle_parser_parsers.conf
                    path: spar-oracle/parser/parsers.conf
                  - key: timestamp.lua
                    path: timestamp.lua
                defaultMode: 0644
  - kind: PersistentVolumeClaim
    apiVersion: v1
    metadata:
      labels:
        app: ${NAME}-${ZONE}
      name: ${NAME}-${ZONE}-${COMPONENT}-logs
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: "50Mi"
      storageClassName: netapp-file-standard
  - kind: ConfigMap
    apiVersion: v1
    metadata:
      name: ${NAME}-${ZONE}-${COMPONENT}-configs
    data:
      filters.conf: |
        [FILTER]
            Name modify
            Match *
            Add agent.type fluentbit
            Add agent.version ${FLUENT_VERSION}
            Add agent.name ${AGENT_NAME}
            Add ecs.version 8.9
            Rename event_sequence event.sequence
            Rename log_file_path log.file.path

        [FILTER]
            Name lua
            Match spar
            script ${FLUENT_CONF_HOME}/timestamp.lua
            time_as_table True
            call append_event_created


        # There is a bug when resolving environment variables with spaces in the value :(
        # So, we have to use Lua script for now
        # Reference: https://github.com/fluent/fluent-bit/issues/1225

        [FILTER]
            Name lua
            Match *
            script ${FLUENT_CONF_HOME}/host_metadata.lua
            time_as_table True
            call add_host_metadata

      fluent-bit.conf: |
        [SERVICE]
            Log_Level       info
            Parsers_File    parsers.conf

        @INCLUDE spar/input/inputs.conf
        @INCLUDE spar-oracle/input/inputs.conf
        @INCLUDE spar/filter/filters.conf
        @INCLUDE spar-oracle/filter/filters.conf
        @INCLUDE filters.conf
        @INCLUDE outputs.conf

      generic_json_parsers.conf: |
        [PARSER]
            Name        generic_json
            Format      json

      host_metadata.lua: |
        -- Space delimited values to array
        function sdv2array(s)
            delimiter = "%S+"
            result = {};
            for match in string.gmatch(s, delimiter) do
                table.insert(result, match);
            end
            return result;
        end

        function isempty(s)
            return s == nil or s == ''
        end

        function copy(obj)
            if type(obj) ~= 'table' then return obj end
            local res = {}
            for k, v in pairs(obj) do res[copy(k)] = copy(v) end
            return res
        end

        function remove_nil_fields(tag, timestamp, record)
            return 2, timestamp, record
        end


        function add_host_metadata(tag, timestamp, record)
            new_record = record
            if isempty(new_record["host"]) then
                new_record["host"] = {}
            end
            local host = new_record["host"]
            if isempty(host["os"]) then
                host["os"] = {}
            end
            host["os"]["name"] = os.getenv("HOST_OS_NAME")
            host["os"]["type"] = os.getenv("HOST_OS_TYPE")
            host["os"]["family"] = os.getenv("HOST_OS_FAMILY")
            host["os"]["kernel"] = os.getenv("HOST_OS_KERNEL")
            host["os"]["full"] = os.getenv("HOST_OS_FULL")
            host["os"]["version"] = os.getenv("HOST_OS_VERSION")
            host["ip"] = os.getenv("HOST_IP")
            host["mac"] = os.getenv("HOST_MAC")
            if os.getenv("HOSTNAME") ~= nil then
                host["name"] = string.lower(os.getenv("HOSTNAME"))
            end
            if os.getenv("HOST_HOSTNAME") ~= nil then
                host["hostname"] = string.lower(os.getenv("HOST_HOSTNAME"))
            end
            host["domain"] = os.getenv("HOST_DOMAIN")
            host["architecture"] = os.getenv("HOST_ARCH")

            if not(isempty(host["ip"])) then
                host["ip"] = sdv2array(host["ip"])
            else
                host["ip"] = nil
            end

            if not(isempty(host["mac"])) then
                host["mac"] = sdv2array(host["mac"])
            else
                host["mac"] = nil
            end

            if not(isempty(host["name"])) then
                host["name"] = sdv2array(host["name"])
            else
                host["name"] = nil
            end

            if not(isempty(host["domain"])) then
                host["domain"] = sdv2array(host["domain"])
            else
                host["domain"] = nil
            end

            return 2, timestamp, new_record
        end

      outputs.conf: |
        [OUTPUT]
            Name  kinesis_streams
            Match *
            region ca-central-1
            stream ${STREAM_NAME}
            role_arn ${ROLE_ARN}
            Retry_Limit 3

      parsers.conf: |
        @INCLUDE generic_json_parsers.conf
        @INCLUDE spar/parser/parsers.conf
        @INCLUDE spar-oracle/parser/parsers.conf

      spar_filter_filters.conf: |
        [FILTER]
            Name lua
            Match spar.*
            script ${FLUENT_CONF_HOME}/timestamp.lua
            time_as_table True
            call append_timestamp

        [FILTER]
            Name modify
            Match spar.*
            Add service.name spar_postgres_api
            Add service.type postgres_api
            Add @metadata.keyAsPath true

      spar-oracle_filter_filters.conf: |
        [FILTER]
            Name lua
            Match spar-oracle.*
            script ${FLUENT_CONF_HOME}/timestamp.lua
            time_as_table True
            call append_timestamp

        [FILTER]
            Name modify
            Match spar-oracle.*
            Add service.name spar_oracle_api
            Add service.type oracle_api
            Add @metadata.keyAsPath true

      spar_input_inputs.conf: |
        [INPUT]
            Name tail
            Tag spar.log
            Buffer_Max_Size 1024k
            Parser spar.json
            Path /logs/postgres-api.log
            Path_Key log_file_path
            Offset_Key event_sequence
            DB /logs/fluent-bit-logs.db
            Read_from_Head True
            Refresh_Interval 15

      spar-oracle_input_inputs.conf: |
        [INPUT]
            Name tail
            Tag spar-oracle.log
            Buffer_Max_Size 1024k
            Parser spar-oracle.json
            Path /logs/oracle-api.log
            Path_Key log_file_path
            Offset_Key event_sequence
            DB /logs/fluent-bit-logs-oracle.db
            Read_from_Head True
            Refresh_Interval 15

      spar_parser_parsers.conf: |
        [PARSER]
            Name spar.json
            Match *
            Format json
            Time_Key @timestamp
            Time_Format %Y-%m-%d %H:%M:%S.%L

      spar-oracle_parser_parsers.conf: |
        [PARSER]
            Name spar-oracle.json
            Match *
            Format json
            Time_Key @timestamp
            Time_Format %Y-%m-%d %H:%M:%S.%L

      timestamp.lua: |
        function append_event_created(tag, timestamp, record)
            new_record = record
            new_record["event.created"] = (os.date("!%Y-%m-%dT%H:%M:%S", timestamp["sec"]) .. '.' .. math.floor(timestamp["nsec"] / 1000000) .. 'Z')
            return 2, timestamp, new_record
        end

        function append_timestamp(tag, timestamp, record)
          new_record = record
          new_record["@timestamp"] = (os.date("!%Y-%m-%dT%H:%M:%S", timestamp["sec"]) .. '.' .. math.floor(timestamp["nsec"] / 1000000) .. 'Z')
          return 2, timestamp, new_record
        end

